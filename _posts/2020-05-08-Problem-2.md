---
layout: post
title: Problem 2
subtitle: A problem on Linear Classification
tags: [SVM, Classifier]
---
# Problem 2 : A problem on Linear Classification

So, we come back to another interesting problem involving some interesting geometry and calculus. This is the famous Support Vector Machine Classifier. While there are many pretty explanations of the math behind the SVM, I'd like to add my two cents. The problem in its simplest form is best defined as finding a "dividing line" between two kinds of points.

<center><img style=" display: block; margin-left: auto; margin-right: auto;width: 50%;" src="../assets/prob2_fig.png"></center>


## Why SVM?

This is simply answered by considering the issue with the general regression setting where we could define a set of parameters $\beta$ which map the input vectors (in the figure $((x1,x2))$) to obtain a classification $f(x) = \beta^T.X $. A quick glance and it becomes obvious that we don't want to deal with high-dimensional data which could quickly get messy.

## So what do we do?

We quickly set forth a goal of obtaining a dividing line. This will be a much faster implementation that will quickly give us the parameters relevant to our line. So, the usual notion from our high school mathematics says a line in 2-D is : $$ y = m.x + c $$ or equivalently, $$ ax + by + c = 0 $$ for some a, b and c.

Extending this to a plane, a plane in 3-D is : $$ ax + by + cz + d = 0 $$ for some a, b, c and d.

And extending this further, using a vector of the coefficients and denoting this as $$ w $$ we get, $$ \mathbf{w^T.X + b = 0} $$ where, we have separately kept the intercept vector $$\mathbf{b}$$. [This is known as a Hyperplane]

Next, we define two Hyperplanes parallel (why?) to this hyperplane namely : $$\mathbf{w^T.X + b = 1}$$ and $$\mathbf{w^T.X + b = -1}$$ the sign of the RHS term essentially means the hyperplanes on the two sides of the hyperplane $$\mathbf{w^T.X + b = 0}$$. The gap between the two hyperplanes on either sides of the hyperplane in the middle is basically the "street width" (you can see it in the 2-D Figure).

Let us label the blue points as 1 and the green ones as -1. And, let us give this label a name : $$ y_i $$ for ith point on the plane. So we want the ends of the "street" to be such that :
$$  w^T.X_i + b >= 1 $$ when $$ y_i = 1  $$ and
$$  w^T.X_i + b <=-1 $$ when $$ y_i = -1 $$ or, in general,
$$  y_i.(w^T.X_i + b) >= 1 $$ for all $$ y_i $$

The term on the left hand side is some sort of a metric which says if the point is well-classified or not. If the inequality is satisfied for all points then we have found one possible hyperplane. But, which hyperplane is the best ? That would obviously be decided by the extent of separation between the data-points. So, we want the LHS of the last inequality to be as large as possible!

In order to ensure that the magnitude of the vector $w$ does not change our metric (it only depends on the direction). The obvious thing is to normalize it i.e. $$ F_i = y_i.(\frac{w^T.X_i}{||w||} + \frac{b}{||w||}) $$ where $$ ||w|| $$ is the magnitude.

So let us formally define the problem at hand:

We need to maximise the $$ \min_{x_1,\dots x_m} F_i $$ under the constrain that $$ F_i >= 1 $$. Quickly, noting that the $$ \min_{x_1,\dots x_m} F_i = \frac{1}{||w||} $$.
