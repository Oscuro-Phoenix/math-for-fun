---
layout: post
title: Problem 28
subtitle: BIC
tags: [Information]
---
# Problem 28 : Bayesian Information Criterion

There's a saying among people who work in theory and modeling -- "All models are wrong but some are useful". I still remember this one day from my pre-final year in undergrad when I was walking back to dorm after a long night during the cultural fest (SF) at IIT-KGP. How do we justify usage of one model over the other? For example, there are several turbulence models -- (one-parameter Prandtl mixing length model, two-parameter $$ \kappa-\varepsilon$$,...) that go into the Reynolds Averaged Navier Stokes (RANS) equations. Instinctively, I had this feeling that the model closer to the "truth" would be the simplest one (least parameters) that could reasonably capture the physics/experimental data. But, I was unsure if this notion of "truth" was true. I already had seen Bayesian and Akaike Information Criteria (BIC,AIC) in my 7th semester "Regression and Time Series Modeling" class and realized that we might have to live with this uncertainty given that the so-called model selection criteria are just heuristics. 
 
When it comes to model selection, the Bayesian Information Criterion (BIC) is often used. I'll go over the derivation of BIC in this blog post.

#  Set-up

We want to find the model $$ M_i $$ that best explains the data $${y_j}$$. The probability of a model given the data in accordance with the Bayes theorem is given by,

$$
P\left(M_i \mid y_1, \ldots, y_n\right)=\frac{P\left(y_1, \ldots, y_n \mid M_i\right) P\left(M_i\right)}{P\left(y_1, \ldots, y_n\right)}
$$

Assuming that all the models are equally likely, the equality implies that maximizing the LHS is the same as maximizing $$P\left(y_1, \ldots, y_n \mid M_i\right) P\left(M_i\right)$$ which is nothing but the marginal likelihood of the data given the $$i^{th}$$ model.

This is where things become interesting. The likelihood function essentially represents the joint probability of observing the dataset $$ {y_j}$$ given the parameters $${\theta_i}$$ in the $$i^{th}$$ model. But the functional form of likelihood is in fact going to involve parameters that capture the probability of observing each data point $$ y_j $$. The overall likelihood would then be given by the product of likelihoods of individual data points (assuming independence for simplicity). For example, if we have a series of data points $${y_j}$$ that we propose are normally distributed with two parameters having mean $$\mu$$ and standard deviation $$\sigma$$, we can define the likelihood to be the product of the probability of each $$ y_j$$ given $$\mu$$ and $$\sigma$$.

Let us denote the likehood function for the $$i^{th}$$ model $$M_i$$ as $$ f(\mathbf{y} \mid \mathbf{\theta}_i)$$ (boldfaced letters represent vectors) and if we have some prior information about the parameters we can include a prior distribution $$ g(\mathbf{\theta})$$. Then the "best" model would be the one that maximizes,

$$
P\left(\mathbf{y} \mid M_i\right)=\int f\left(\mathbf{y} \mid \boldsymbol{\theta}_i\right) g_i\left(\boldsymbol{\theta}_i\right) d \boldsymbol{\theta}_i=\int \exp \left(\log \left(f\left(\mathbf{y} \mid \boldsymbol{\theta}_i\right) g_i\left(\boldsymbol{\theta}_i\right)\right)\right) d \boldsymbol{\theta}_i
$$


Let us now assume the likelihood function is strongly peaked at the MLE. This will allow us to use Laplace's method of approximating integral, a classic trick for sharply peaked functions.


### This completes the proof.
