---
layout: post
title: Problem 31
subtitle: LLMs
tags: [AI]
---
# Problem 31 : LLMs - What's under the hood?

[Reuters](https://www.reuters.com/world/ai-defined-2023-bullets-ballots-will-shape-2024-2023-12-04/) reported artificial intelligence (AI) as a defining feature of 2023. AI is an umbrella term that encompasses a bunch of capabilities that demonstrate machine intelligence within which Generative AI which is a specific kind of Deep Learning (DL) technology has been making the rounds. This has been so profound that Bloomberg Intelligence (BI) reported a CAGR of 42% for the Generative AI market (combining infrastructure, services and products) to as high as 1.32 Trillion USD by 2032! Whether its Google's Bard, xAI's Grock or OpenAI's ChatGPT, all of these Large Language Models are Generative Pre-Trained Transformers (GPTs) that have over tens of billions (175 billion for ChatGPT) of parameters and are trained on a large corpus of text. 

Naturally, I got interested in understanding the true ramifications of such models and being more of a fundamentalist this made me want to see what is happening under the hood. I wanted to see if I could explain in a style that retains some of the essential math while helping build an understanding of what it is that makes these models seemingly powerful.


- Generating Embeddings - Words to Vectors 

It is easy to realize that we cannot simply feed words into a neural network, we need numbers. A naive approach to achieve this would be an array as large as
the whole dictionary of words in the corpus of text that would have binary values for mapping a word to a unique array. This approach is called "Bag of Words" and could quickly become cumbersome 
for a large database where each word would require such an array. There is a much better way of dealing with this..

Before we begin, a quick review of feed forward neural network is necessary. A typical feed forward neural network is composed of an input layer that for now say takes in a vector valued input. Subsequent information passage happens through the hidden layers that reside in between the input and output layer. On the output side, an output vector is computed. Nodes in the NN are connected through real  valued weights that can be cleanly represnted through weight matrices (W) with dimensionality n x m for a connectivity between simultaneous layers with n and m nodes each. 

Now, let us say that we want to somehow capture all these words while also capturing "information" of the context that these words typically appear in our corpus. Alternatively, we want to project the words onto a "meaning space" which can be used to represent words with fewer dimensions. To do this, we need some notion of "closeness" in the "meaning space" and this is precisely where a simple feed forward NN trained for a task that forces it to learn this notion of "closeness". One such task is predicting word that appears amidst the words around it (prediction of the word based on context) and another is predicting words around a specific word (prediction of context based on the word). In either case, we are predicting "words" that appear in a certain context. Read more about Skip-gram and Continuous Bag of Words.

Training of such a NN that predicts neighboring words once done using the naive "Bag of Words" approach, provides an entirely new way to represent words. How? -- By simply looking at the output of the last hidden layer.
Why? --  The output layer in this case uses softmax to return a probability vector representing probability of the "neighboring" word. The numbers that get fed to the output layer would be representative of "closeness". 

Minor note: In NLP, not only whole words but also bits and pieces of words (tokens) are converted to real-valued vectors (embeddings) are then used as input 

- We have a way to change words/tokens to numbers but now what?

Can we use feed forward NN? Evidently, a simple feed forward neural network cannot synthesize text because the task of predicting words requires context of the previous word. Recurrent Neural Networks (RNNs) ? Probably, but that would only consider the previous word with information passage happening through an internal state variable passed on from the previous network in the series. The answer seems to be something known as "self-attention" or an "attention head". Self-attention layers serve as building block of transformers.


- Attention is all you need (2017)

The quest to build a building block led to some apparently intuitive ways to dealing with the input embeddings. Simple matrix multiplication with the embeddings i.e., say, a list of embeddings typically denoted as $$ x_1, x_2, x_3, \dots $$. Concatenated set of these embeddings $$X$$ is then used to compute three new internal vectors : Queries (Q), Keys (K) and Values (V). I am not sure about the exact role of each vector. Mathematically, each vector Q, K and V is formed out of corresponding learned matrices $$W_Q$$, $$W_K$$ and $$W_V$$ by multiplying each of them with $$X$$. The high level idea seems to be that Q and K are linear transformations of the embeddings such that together they tell how "appropriate" a token is at a given position. Therefore, the normalized product $$\frac{QK^T}{\sqrt{d_k}}$$ is fed to softmax in the last step of a self-attention block and each row of this is used to perform a weighted sum of the Values (V) corresponding to the $$i^{th}$$ token. Mathematically, these are summarized below,
