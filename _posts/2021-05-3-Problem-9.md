---
layout: post
title: Problem 9
subtitle: A Measure of Distance?
tags: [Probability, Information Theory]
---
# Problem 9 : Kullback-Leibler (KL) Divergence

I had never heard about the KL Divergence until the Summer of 2019 when I happened to work on a summer project on modeling of the ATP Synthase Enzyme. I was new to the subject and came across the expression when I was reading about the parallels between statmech and information theory. For reasons that will become clear later, it is also known as relative entropy. Let us consider the discrete probability distribution here and prove the following inequality -

$$
D_{\mathrm{KL}}(P \| Q)=\sum_{x \in \mathcal{X}} P(x) \ln \left(\frac{P(x)}{Q(x)}\right) >= 0
$$

for probability distributions $$ P $$ and $$ Q $$ over ensemble containing $$ x $$ in $$ \mathcal{X} $$.

# Intuitive Solution

Let us assume that we have a message transmitter sending out discrete signals that take on values ranging form $$ x1, x2, ... xM $$. Each of these signals are sent out with a probability given by $$ p1, p2,....., pM $$. Before we proceed any further, we need to show that the minimum number of bits needed to encode $$ xi $$ having probability $$ pi $$ is given by $$ -log(\frac{1}{pi}) $$. 
