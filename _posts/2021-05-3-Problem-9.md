---
layout: post
title: Problem 9
subtitle: A Measure of Distance?
tags: [Probability, Information Theory]
---
# Problem 9 : Kullback-Leibler (KL) Divergence

I had never heard about the KL Divergence until the Summer of 2019 when I happened to work on a summer project on modeling of the ATP Synthase Enzyme. I was new to the subject and came across the expression when I was reading about the parallels between statmech and information theory. For reasons that will become clear later, it is also known as relative entropy. Let us consider the discrete probability distribution here and prove the following inequality -

$$
D_{\mathrm{KL}}(P \| Q)=\sum_{x \in \mathcal{X}} P(x) \ln \left(\frac{P(x)}{Q(x)}\right) >= 0
$$

for probability distributions $$ P $$ and $$ Q $$ over ensemble of $$ x $$ in $$ \mathcal{X} $$.
# Solution
