---
layout: post
title: Problem 18
subtitle: Gaussian Mixture Model (EM Algorithm)
tags: [Bayesian Learning]
---
# Problem 18: Mixture of Gaussians - A Soft Classification Approach (ft. Jensen's Inequality)

The Bayesian framework for solving problems has already turned into a phenomenon. A hard classification approach like the K-means algorithm does not let you discuss about uncertainty or certainty with which you can say that a data point belongs to a specific class/group. This calls for a probabilistic approach and the Gaussian Mixture Model (GMM) which has been around for more than three decades. 

<figure>
<center><img style=" display: block; margin-left: auto; margin-right: auto;width: 75%;" src="../assets/prob18_fig1.png">
<figcaption>A simplified case with GMM classification over a single dimension</figcaption>
 </center>


The figure given above shows a simple case where we consier classification among three classes over a single dimension. This dimension could be a characteristic like petal width for the Iris classification problem. It may be noted that the some of the parameters which are clearly visible in the figure include the mean (vector) and variance-covariance (matrix) of each of the elemental Gaussian distributions. 

Let us now introduce a latent variable ($ t_i $) the value of which for a given data point and parameters represents the Gaussian to which the data point belongs. In mathematical terms, we want to say that :

$$ P\left(t_{i}=j \mid x_{i}, \Theta\right)  $$ is the probability that $$ x_{i} $$ belongs to the $$ j^{th} $$ Gaussian.

We want our GMM to explain the data really well. This really means that we want the probability of the data (X) given the parameters ($$\Theta$$) to be as high as possible. Experienced folks would now realize that we are hinting towards the Maximum Likelihood Estimation (MLE).

Accordingly, we can write our objective as the maximization of the log of this distribution of the data given parameters. We take the logarithm to make the math easier. The objective can be mathematically expressed as follows:

$$
\max _{\Theta} \log P(X \mid \Theta)=\max _{\Theta} \log \left(\prod_{i} P\left(x_{i} \mid \Theta\right)\right)=\max _{\Theta} \sum_{i} \log \left(P\left(x_{i} \mid \Theta\right)\right)
$$


